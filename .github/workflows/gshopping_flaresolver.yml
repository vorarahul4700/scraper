name: Google Shopping Scraper (FlareSolverr Pool)

on:
  workflow_dispatch:
    inputs:
      input_filename:
        description: "Input CSV filename on FTP"
        required: true
        default: "google_shopping.csv"
        type: string
      total_chunks:
        description: "Number of chunks to split into (0 = all in one run)"
        required: true
        default: "4"
        type: string
      flaresolver_hosts:
        description: "Number of FlareSolverr hosts per job"
        required: true
        default: "5"
        type: string
      max_retries:
        description: "Retries per product (0 = same as host count)"
        required: true
        default: "0"
        type: string

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL_CHUNKS=${{ github.event.inputs.total_chunks || 4 }}

          if [ "$TOTAL_CHUNKS" -le 0 ]; then
            MATRIX='[{"chunk_id":1}]'
          else
            MATRIX="["
            for ((i=1;i<=TOTAL_CHUNKS;i++)); do
              MATRIX+="{\"chunk_id\":$i},"
            done
            MATRIX="${MATRIX%,}]"
          fi

          echo "matrix=$MATRIX" >> "$GITHUB_OUTPUT"
          echo "Matrix: $MATRIX"

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - name: Start FlareSolverr pool
        id: flaresolverr_pool
        run: |
          INSTANCES="${{ github.event.inputs.flaresolver_hosts || 5 }}"
          if ! [[ "$INSTANCES" =~ ^[0-9]+$ ]] || [ "$INSTANCES" -lt 1 ]; then
            INSTANCES=1
          fi

          BASE_PORT=8191
          URLS=""
          HEALTHY_INSTANCES=0

          echo "Starting $INSTANCES FlareSolverr instances..."
          for ((i=0; i<INSTANCES; i++)); do
            PORT=$((BASE_PORT + i))
            NAME="flaresolverr_${PORT}"
            docker run -d --name "$NAME" --cap-add=SYS_ADMIN -p "${PORT}:8191" ghcr.io/flaresolverr/flaresolverr:latest >/dev/null || true
          done

          for ((i=0; i<INSTANCES; i++)); do
            PORT=$((BASE_PORT + i))
            NAME="flaresolverr_${PORT}"
            URL="http://localhost:${PORT}/v1"
            READY=0

            for attempt in {1..40}; do
              if curl -s -f -X POST "$URL" -H "Content-Type: application/json" -d '{"cmd":"sessions.list"}' > /dev/null 2>&1; then
                READY=1
                break
              fi
              sleep 1
            done

            if [ "$READY" -eq 1 ]; then
              URLS="${URLS:+$URLS,}$URL"
              HEALTHY_INSTANCES=$((HEALTHY_INSTANCES + 1))
            else
              docker rm -f "$NAME" >/dev/null 2>&1 || true
            fi
          done

          if [ "$HEALTHY_INSTANCES" -lt 1 ]; then
            echo "No healthy FlareSolverr instances available"
            exit 1
          fi

          echo "instances=$HEALTHY_INSTANCES" >> "$GITHUB_OUTPUT"
          echo "urls=$URLS" >> "$GITHUB_OUTPUT"
          echo "FlareSolverr URL list: $URLS"

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install pandas==2.1.4 requests==2.31.0 beautifulsoup4==4.12.2 lxml==4.9.3

      - name: Run scraper
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
          FLARESOLVERR_URLS: ${{ steps.flaresolverr_pool.outputs.urls }}
        run: |
          INPUT_FILE="${{ github.event.inputs.input_filename || 'google_shopping.csv' }}"
          TOTAL_CHUNKS="${{ github.event.inputs.total_chunks || 4 }}"
          MAX_RETRIES="${{ github.event.inputs.max_retries || 0 }}"

          python -u gshopping/gscrapperci_flaresolver.py \
            --chunk-id ${{ matrix.chunk_id }} \
            --total-chunks "$TOTAL_CHUNKS" \
            --input-file "$INPUT_FILE" \
            --flaresolver-urls "$FLARESOLVERR_URLS" \
            --max-retries "$MAX_RETRIES"

      - name: Upload chunk results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: fs-chunk-${{ matrix.chunk_id }}
          path: output/
          retention-days: 1

      - name: Stop FlareSolverr pool
        if: always()
        run: |
          docker ps -a --format '{{.Names}}' | grep '^flaresolverr_' | xargs -r docker rm -f

  merge:
    needs: scrape
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install pandas
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.1.4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge CSV files
        run: |
          cat > merge_results.py <<EOF
          import os
          import pandas as pd
          from datetime import datetime

          completed_product_files = []
          completed_seller_files = []
          remaining_files = []

          for root, _, files in os.walk("chunks"):
              for f in files:
                  path = os.path.join(root, f)
                  if f.endswith(".csv") and "completed_products" in f:
                      completed_product_files.append(path)
                  elif f.endswith(".csv") and "completed_sellers" in f:
                      completed_seller_files.append(path)
                  elif f.endswith(".csv") and "remaining" in f:
                      remaining_files.append(path)

          ts = datetime.now().strftime("%Y%m%d_%H%M%S")

          if completed_product_files:
              df = pd.concat((pd.read_csv(f) for f in completed_product_files), ignore_index=True)
              if "product_id" in df.columns:
                  df.sort_values("product_id", inplace=True)
              df.to_csv(f"merged_completed_products_{ts}.csv", index=False)

          if completed_seller_files:
              df = pd.concat((pd.read_csv(f) for f in completed_seller_files), ignore_index=True)
              if set(["product_id", "seller"]).issubset(df.columns):
                  df.sort_values(["product_id", "seller"], inplace=True)
              df.to_csv(f"merged_completed_sellers_{ts}.csv", index=False)

          if remaining_files:
              df = pd.concat((pd.read_csv(f) for f in remaining_files), ignore_index=True)
              if "product_id" in df.columns:
                  df.sort_values("product_id", inplace=True)
              df.to_csv(f"merged_remaining_{ts}.csv", index=False)
          EOF

          python merge_results.py

      - name: Upload merged files to FTP
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
        run: |
          set -e
          sudo apt-get update
          sudo apt-get install -y lftp

          upload () {
            FILE="$1"
            cat >/tmp/upload.lftp <<EOF
          set ftp:ssl-allow no
          open $FTP_HOST
          user $FTP_USER $FTP_PASS
          mkdir -p $FTP_PATH
          cd $FTP_PATH
          put $FILE
          bye
          EOF
            lftp -f /tmp/upload.lftp
            rm -f /tmp/upload.lftp
          }

          shopt -s nullglob
          for f in merged_completed_products_*.csv; do upload "$f"; done
          for f in merged_completed_sellers_*.csv; do upload "$f"; done
          for f in merged_remaining_*.csv; do upload "$f"; done

      - name: Upload merged artifacts
        uses: actions/upload-artifact@v4
        with:
          name: merged-fs-results
          path: |
            merged_completed_products_*.csv
            merged_completed_sellers_*.csv
            merged_remaining_*.csv
          retention-days: 7
