name: Google Shopping Scraper

on:
  workflow_dispatch:
    inputs:
      input_filename:
        description: "Input CSV filename on FTP"
        required: true
        default: "google_shopping.csv"
        type: string
      total_chunks:
        description: "Number of chunks per round"
        required: true
        default: "20"
        type: string
      run_depth:
        description: "Current recursive run depth"
        required: true
        default: "1"
        type: string
      max_depth:
        description: "Maximum recursive run depth"
        required: true
        default: "10"
        type: string

permissions:
  contents: read
  actions: write

jobs:
  plan:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.matrix.outputs.matrix }}
    steps:
      - id: matrix
        run: |
          TOTAL_CHUNKS=${{ github.event.inputs.total_chunks || 20 }}

          MATRIX="["
          for ((i=1;i<=TOTAL_CHUNKS;i++)); do
            MATRIX+="{\"chunk_id\":$i},"
          done
          MATRIX="${MATRIX%,}]"

          echo "matrix=$MATRIX" >> "$GITHUB_OUTPUT"
          echo "Matrix: $MATRIX"

  scrape:
    needs: plan
    runs-on: ubuntu-22.04
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.plan.outputs.matrix) }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install Chrome and dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            wget \
            gnupg \
            ca-certificates \
            unzip \
            xvfb \
            libxss1 \
            libappindicator3-1 \
            libindicator7 \
            libnss3 \
            libnspr4 \
            libatk-bridge2.0-0 \
            libgtk-3-0 \
            libx11-xcb1 \
            libxcomposite1 \
            libxrandr2 \
            libgbm1 \
            libasound2 \
            fonts-liberation \
            libu2f-udev

          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

          google-chrome-stable --version

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install \
            selenium==4.18.1 \
            undetected-chromedriver==3.5.5 \
            webdriver-manager \
            pandas==2.1.4 \
            beautifulsoup4==4.12.2 \
            requests==2.31.0 \
            lxml==4.9.3 \
            fake-useragent==1.4.0 \
            python-dateutil==2.8.2 \
            speechrecognition==3.10.1 \
            pydub==0.25.1 \
            chromedriver-autoinstaller==0.6.4

      - name: Run scraper chunk
        env:
          DISPLAY: :99
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
        run: |
          Xvfb :99 -screen 0 1920x1080x24 &
          sleep 3

          INPUT_FILE="${{ github.event.inputs.input_filename || 'google_shopping.csv' }}"
          TOTAL_CHUNKS="${{ github.event.inputs.total_chunks || 20 }}"

          python -u gshopping/gscrapperci.py \
            --chunk-id ${{ matrix.chunk_id }} \
            --total-chunks "$TOTAL_CHUNKS" \
            --input-file "$INPUT_FILE"

      - name: Upload chunk results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: chunk-${{ matrix.chunk_id }}
          path: output/
          retention-days: 2

  merge:
    needs: scrape
    runs-on: ubuntu-latest
    outputs:
      has_remaining: ${{ steps.merge_results.outputs.has_remaining }}
      next_input_filename: ${{ steps.merge_results.outputs.next_input_filename }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install pandas
        run: |
          python -m pip install --upgrade pip
          pip install pandas==2.1.4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: chunks

      - name: Merge round files and update cumulative FTP files
        id: merge_results
        env:
          FTP_HOST: ${{ secrets.FTP_HOST }}
          FTP_PORT: ${{ secrets.FTP_PORT }}
          FTP_USER: ${{ secrets.FTP_USER }}
          FTP_PASS: ${{ secrets.FTP_PASS }}
          FTP_PATH: ${{ secrets.FTP_PATH }}
          RUN_DEPTH: ${{ github.event.inputs.run_depth || '1' }}
        run: |
          python - <<'PY'
          import os
          import ftplib
          import pandas as pd

          ftp_host = os.environ.get("FTP_HOST")
          ftp_user = os.environ.get("FTP_USER")
          ftp_pass = os.environ.get("FTP_PASS")
          ftp_port = int(os.environ.get("FTP_PORT", "21"))
          ftp_path = os.environ.get("FTP_PATH", "/scrap/")
          run_depth = int(os.environ.get("RUN_DEPTH", "1"))

          product_files = []
          seller_files = []
          remaining_files = []

          for root, _, files in os.walk("chunks"):
              for f in files:
                  p = os.path.join(root, f)
                  if f.endswith(".csv") and "product_info_" in f and "chunk" in f:
                      product_files.append(p)
                  elif f.endswith(".csv") and "seller_info_" in f and "chunk" in f:
                      seller_files.append(p)
                  elif f.endswith(".csv") and "gshopping_remaining_" in f and "chunk" in f:
                      remaining_files.append(p)

          def merge_files(paths, out_name, sort_cols=None):
              frames = []
              for p in sorted(paths):
                  try:
                      df = pd.read_csv(p)
                      if not df.empty:
                          frames.append(df)
                  except Exception as e:
                      print(f"Warning reading {p}: {e}")
              if not frames:
                  return None
              df = pd.concat(frames, ignore_index=True)
              if sort_cols:
                  cols = [c for c in sort_cols if c in df.columns]
                  if cols:
                      df = df.sort_values(cols)
              df.to_csv(out_name, index=False)
              return out_name

          round_products = merge_files(product_files, "round_products.csv", ["product_id"])
          round_sellers = merge_files(seller_files, "round_sellers.csv", ["product_id", "seller"])
          round_remaining = merge_files(remaining_files, "gshopping_remaining.csv", ["product_id"])

          if not all([ftp_host, ftp_user, ftp_pass]):
              raise RuntimeError("FTP credentials are missing")

          ftp = ftplib.FTP()
          ftp.connect(ftp_host, ftp_port)
          ftp.login(ftp_user, ftp_pass)
          ftp.set_pasv(True)

          if ftp_path and ftp_path != '/':
              parts = [p for p in ftp_path.strip('/').split('/') if p]
              cur = ''
              for d in parts:
                  cur += '/' + d
                  try:
                      ftp.cwd(cur)
                  except Exception:
                      ftp.mkd(cur)
                      ftp.cwd(cur)

          def ftp_download_if_exists(remote_name, local_name):
              try:
                  with open(local_name, 'wb') as f:
                      ftp.retrbinary(f"RETR {remote_name}", f.write)
                  return True
              except Exception:
                  if os.path.exists(local_name):
                      os.remove(local_name)
                  return False

          def ftp_upload(local_name, remote_name):
              with open(local_name, 'rb') as f:
                  ftp.storbinary(f"STOR {remote_name}", f)
              print(f"Uploaded {remote_name}")

          def update_cumulative(round_file, cumulative_remote, out_local, sort_cols):
              if not round_file or not os.path.exists(round_file):
                  return None
              base_exists = ftp_download_if_exists(cumulative_remote, "_prev.csv")
              frames = []
              if base_exists:
                  try:
                      prev = pd.read_csv("_prev.csv")
                      if not prev.empty:
                          frames.append(prev)
                  except Exception as e:
                      print(f"Warning reading previous {cumulative_remote}: {e}")
              cur = pd.read_csv(round_file)
              if not cur.empty:
                  frames.append(cur)
              if not frames:
                  return None
              merged = pd.concat(frames, ignore_index=True)
              cols = [c for c in sort_cols if c in merged.columns]
              if cols:
                  merged = merged.sort_values(cols)
              merged.to_csv(out_local, index=False)
              ftp_upload(out_local, cumulative_remote)
              return out_local

          final_products_local = update_cumulative(
              round_products,
              "gshopping_products_final.csv",
              "gshopping_products_final.csv",
              ["product_id"],
          )
          final_sellers_local = update_cumulative(
              round_sellers,
              "gshopping_sellers_final.csv",
              "gshopping_sellers_final.csv",
              ["product_id", "seller"],
          )

          has_remaining = "false"
          next_input_filename = ""

          if round_remaining and os.path.exists(round_remaining):
              rem_df = pd.read_csv(round_remaining)
              if not rem_df.empty:
                  has_remaining = "true"
                  next_input_filename = f"gshopping_remaining_depth_{run_depth}.csv"
                  ftp_upload(round_remaining, next_input_filename)

          ftp.quit()

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
              f.write(f"has_remaining={has_remaining}\n")
              f.write(f"next_input_filename={next_input_filename}\n")

          print(f"has_remaining={has_remaining}")
          print(f"next_input_filename={next_input_filename}")
          PY

      - name: Upload merged artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: merged-round-depth-${{ github.event.inputs.run_depth || '1' }}
          path: |
            round_products.csv
            round_sellers.csv
            gshopping_remaining.csv
            gshopping_products_final.csv
            gshopping_sellers_final.csv
          retention-days: 7

  dispatch_next_round:
    needs: merge
    if: ${{ needs.merge.outputs.has_remaining == 'true' && fromJSON(github.event.inputs.run_depth || '1') < fromJSON(github.event.inputs.max_depth || '10') }}
    runs-on: ubuntu-latest

    steps:
      - name: Trigger next round workflow
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const runDepth = Number('${{ github.event.inputs.run_depth || '1' }}');
            const nextDepth = runDepth + 1;
            const totalChunks = '${{ github.event.inputs.total_chunks || '20' }}';
            const maxDepth = '${{ github.event.inputs.max_depth || '10' }}';
            const nextInput = '${{ needs.merge.outputs.next_input_filename }}';

            await github.rest.actions.createWorkflowDispatch({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'gshopping.yml',
              ref: context.ref.replace('refs/heads/', ''),
              inputs: {
                input_filename: nextInput,
                total_chunks: totalChunks,
                run_depth: String(nextDepth),
                max_depth: maxDepth,
              },
            });

            core.info(`Triggered next round depth=${nextDepth} with input=${nextInput}`);

  notify:
    needs: [merge, dispatch_next_round]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Summary
        run: |
          echo "Workflow finished"
          echo "Run depth: ${{ github.event.inputs.run_depth || '1' }}"
          echo "Has remaining: ${{ needs.merge.outputs.has_remaining }}"
          echo "Next input: ${{ needs.merge.outputs.next_input_filename }}"
          echo "Run: https://github.com/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID"
